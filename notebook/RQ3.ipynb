{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aa625625a163e941296ceca1a008e8fd8c19fa547a3e67e20e2e7834074e6713"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RQ3 Questions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from tweepy import Cursor\n",
    "from tweepy import API\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tweepy import OAuthHandler\n",
    "import networkx as nx\n",
    "import igraph\n",
    "from scipy.sparse import csr_matrix\n",
    "import collections\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/processed_retweets.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw_tweets = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_retweets = df_raw_tweets[df_raw_tweets[\"text\"].apply(lambda x: x[:2]) == \"RT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createRetweetsGraph(df_retweets):\n",
    "    g = igraph.Graph()\n",
    "    users = []\n",
    "    edges = []\n",
    "    user2ret_user = {}\n",
    "    for index, row in df_retweets.iterrows():\n",
    "        user = row[\"user\"][\"screen_name\"]\n",
    "        if(user not in users):\n",
    "            users.append(user)\n",
    "        mentioned_users = list(filter(lambda word: word[0]=='@', row['text'].split()))\n",
    "        if(len(mentioned_users)>0):\n",
    "            ret_user = mentioned_users[0][1:-1]  \n",
    "            if(ret_user not in users):\n",
    "                users.append(ret_user)  \n",
    "            edges.append((user, ret_user))\n",
    "    g.add_vertices(users)\n",
    "    g.add_edges(edges)\n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "original_graph = createRetweetsGraph(df_retweets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "40092"
      ]
     },
     "metadata": {},
     "execution_count": 66
    }
   ],
   "source": [
    "original_graph.vcount()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "34169"
      ]
     },
     "metadata": {},
     "execution_count": 67
    }
   ],
   "source": [
    "original_graph.ecount()"
   ]
  },
  {
   "source": [
    "## Test and train data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_nodes_at_distance_2(graph):\n",
    "    all_potential_recommendations = set()\n",
    "    \n",
    "    for n1 in graph.vs:\n",
    "\n",
    "        nodes_at_most_distant_1 = set(graph.neighborhood(vertices=n1, order=1))\n",
    "        nodes_at_most_distant_2 = set(graph.neighborhood(vertices=n1, order=2))\n",
    "\n",
    "        only_nodes_at_distance_2 = nodes_at_most_distant_2 - nodes_at_most_distant_1\n",
    "        \n",
    "        if len(only_nodes_at_distance_2) > 0:\n",
    "        \n",
    "            for n2 in only_nodes_at_distance_2:\n",
    "                n1_index = n1.index\n",
    "                \n",
    "                all_potential_recommendations.add((n1_index, n2))\n",
    "            \n",
    "    return all_potential_recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_distance_2 = find_nodes_at_distance_2(original_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of edges to select as test-set\n",
    "p = 0.2\n",
    "\n",
    "# graphsize\n",
    "N = len(original_graph.es)\n",
    "\n",
    "# idxs of all the edges\n",
    "all_idxs = range(N)\n",
    "\n",
    "# sample idxs of edges through the function \"choice\"\n",
    "test_idxs = np.random.choice(a=all_idxs, size=int(p*N), replace=False)\n",
    "\n",
    "D2 = np.random.permutation(list(nodes_distance_2))[:(len(test_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1315"
      ]
     },
     "metadata": {},
     "execution_count": 23
    }
   ],
   "source": [
    "len(test_idxs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set()\n",
    "for idx, one_edge in enumerate(original_graph.es):\n",
    "    n1 = one_edge.source\n",
    "    n2 = one_edge.target\n",
    "\n",
    "    if idx in test_idxs:\n",
    "        ground_truth.add((n1, n2, 1))\n",
    "for n1, n2 in D2:\n",
    "    ground_truth.add((n1, n2, 0))"
   ]
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## ALS"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "import implicit\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we get the adjacency matrix data\n",
    "M = original_graph.get_adjacency().data\n",
    "M = csr_matrix(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 5/5 [00:02<00:00,  2.04it/s, loss=0.00015]\n"
     ]
    }
   ],
   "source": [
    "model = implicit.als.AlternatingLeastSquares(factors=10, calculate_training_loss=True,  iterations=5)\n",
    "# train the model\n",
    "model.fit(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_ALS(testset, model):\n",
    "    \n",
    "    # initialize the empty list\n",
    "    all_predictions = []\n",
    "\n",
    "    # scroll the obs\n",
    "    for n1,n2, w in testset:\n",
    "        \n",
    "        array_n1 = model.user_factors[n1,:]\n",
    "        array_n2 = model.item_factors[n2,:]\n",
    "\n",
    "        one_p = np.dot(array_n1, array_n2)\n",
    "\n",
    "        all_predictions.append(one_p)\n",
    "        \n",
    "    return all_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate the predictions\n",
    "df_test = pd.DataFrame(list(ground_truth), columns=[\"n1\",\"n2\", \"edge\"])\n",
    "all_predictions = predict_ALS(df_test.values, model)\n",
    "\n",
    "# add predictions to df\n",
    "df_test[\"rating\"] = all_predictions\n",
    "\n",
    "# convert predictions to binary values: 0 don't add the edge, 1 add it.\n",
    "df_test[\"rating\"] = df_test[\"rating\"].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5731857318573186"
      ]
     },
     "metadata": {},
     "execution_count": 275
    }
   ],
   "source": [
    "# number of observations matched by the prediction\n",
    "right_predictions = len(df_test[df_test[\"rating\"] == df_test[\"edge\"]])\n",
    "\n",
    "# accuracy\n",
    "right_predictions/len(df_test)"
   ]
  },
  {
   "source": [
    "## ADA"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "def compute_ADA(u,v, graph):\n",
    "    \n",
    "    # set of neighbors of u\n",
    "    outlinks_from_u = graph.neighbors(u)\n",
    "\n",
    "    # set of neighbors of v\n",
    "    inlinks_to_v = graph.neighbors(v)\n",
    "\n",
    "    \n",
    "    # set Z of neighbors of both\n",
    "    bridges = set(outlinks_from_u).intersection(inlinks_to_v)\n",
    "\n",
    "    # degree of nodes in set Z\n",
    "    deg_ = [graph.degree(n) for n in bridges]\n",
    "    \n",
    "    # computing the reciprocal in log-scale\n",
    "    out = [1./np.log2(dd+1) for dd in deg_]\n",
    "\n",
    "    return sum(out)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 281,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [],
   "source": [
    "ADA_test = pd.DataFrame(list(ground_truth), columns=[\"n1\",\"n2\", \"edge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "allscores= []\n",
    "for n1,n2, w in ADA_test.values:\n",
    "    score = compute_ADA(n1,n2,original_graph)\n",
    "    allscores.append(score)\n",
    "ADA_test[\"rating\"] = allscores\n",
    "ADA_test[\"rating\"] = ADA_test[\"rating\"].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5014350143501435"
      ]
     },
     "metadata": {},
     "execution_count": 290
    }
   ],
   "source": [
    "right_predictions = len(ADA_test[ADA_test[\"rating\"] == ADA_test[\"edge\"]])\n",
    "\n",
    "# accuracy\n",
    "right_predictions/len(ADA_test)"
   ]
  },
  {
   "source": [
    "## Page rank"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_test = pd.DataFrame(list(ground_truth), columns=[\"n1\",\"n2\", \"edge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pagerank = enumerate(original_graph.personalized_pagerank(reset_vertices=10))\n",
    "allpredictions= []\n",
    "for n1,n2, w in PR_test.values:\n",
    "    pagerank = enumerate(original_graph.personalized_pagerank(reset_vertices=n1))\n",
    "    for i, val in pagerank:\n",
    "        if(i == n2):\n",
    "            allpredictions.append(val)\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [],
   "source": [
    "PR_test[\"rating\"] = allpredictions\n",
    "PR_test[\"rating\"] = PR_test[\"rating\"].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.5038950389503895"
      ]
     },
     "metadata": {},
     "execution_count": 311
    }
   ],
   "source": [
    "right_predictions = len(PR_test[PR_test[\"rating\"] == PR_test[\"edge\"]])\n",
    "\n",
    "# accuracy\n",
    "right_predictions/len(PR_test)"
   ]
  },
  {
   "source": [
    "# Proposed algorithm"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "## Logistic regresion"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fraction of edges to select as test-set\n",
    "p = 0.2\n",
    "# graphsize\n",
    "N = len(original_graph.es)\n",
    "\n",
    "# idxs of all the edges\n",
    "all_idxs = range(N)\n",
    "\n",
    "# sample idxs of edges through the function \"choice\"\n",
    "test_idxs = np.random.choice(a=all_idxs, size=int(p*N), replace=False)\n",
    "#train idx of edges\n",
    "train_idx = []\n",
    "for i in range(N):\n",
    "    if(i not in test_idxs):\n",
    "        train_idx.append(i)\n",
    "D2 = np.random.permutation(list(nodes_distance_2))[:(len(test_idxs))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ground_truth = set()\n",
    "for idx, one_edge in enumerate(original_graph.es):\n",
    "    n1 = one_edge.source\n",
    "    n2 = one_edge.target\n",
    "\n",
    "    if idx in test_idxs:\n",
    "        ground_truth.add((n1, n2, 1))\n",
    "for n1, n2 in D2:\n",
    "    ground_truth.add((n1, n2, 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "for u,v,w in ground_truth:\n",
    "    trainX.append((u,v))\n",
    "    trainY.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainX = []\n",
    "trainY = []\n",
    "for idx, one_edge in enumerate(original_graph.es):\n",
    "    n1 = one_edge.source\n",
    "    n2 = one_edge.target\n",
    "    link = 1\n",
    "    if idx in test_idxs:\n",
    "        trainX.append((n1, n2))\n",
    "        trainY.append(0)\n",
    "    else:\n",
    "        trainX.append((n1, n2))\n",
    "        trainY.append(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_test = pd.DataFrame(list(ground_truth), columns=[\"n1\",\"n2\", \"edge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testX = []\n",
    "testY = []\n",
    "for u,v,w in ground_truth:\n",
    "    testX.append((u,v))\n",
    "    testY.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "M = original_graph.get_adjacency().data\n",
    "M = csr_matrix(M)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "LogisticRegression(class_weight='balanced')"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "lr = LogisticRegression(class_weight=\"balanced\")\n",
    "lr.fit(trainX,trainY )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "allpredictions = lr.predict_proba(testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_test[\"rating\"] = allpredictions[:,1]\n",
    "LR_test[\"rating\"] = LR_test[\"rating\"].apply(lambda x: round(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.694358748845214\n"
     ]
    }
   ],
   "source": [
    "right_predictions = len(LR_test[LR_test[\"rating\"] == LR_test[\"edge\"]])\n",
    "# accuracy\n",
    "right_predictions/len(LR_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}