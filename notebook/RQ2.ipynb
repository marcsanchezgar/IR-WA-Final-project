{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.7.8 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "aa625625a163e941296ceca1a008e8fd8c19fa547a3e67e20e2e7834074e6713"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# RQ2 Questions"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package punkt to\n[nltk_data]     C:\\Users\\Marc\\AppData\\Roaming\\nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 1
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "import nltk\n",
    "import re\n",
    "from array import array\n",
    "from collections import defaultdict\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "import math\n",
    "from numpy import linalg as la\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "source": [
    "Importing data"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/processed_tweets.json\", \"rb\") as f:\n",
    "    data = f.readlines()\n",
    "    data = [json.loads(str_) for str_ in data[:30000]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets = pd.DataFrame.from_records(data)"
   ]
  },
  {
   "source": [
    "This function is to process the query"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def processQuery(line):        \n",
    "    stemming = PorterStemmer()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    line=  line.lower()\n",
    "    line = re.sub('[^a-zA-Z]', ' ', line )                                 ## Transform in lowercase\n",
    "    line=  line.split()                                 ## Tokenize the text to get a list of terms\n",
    "    line=[word for word in line if word not in stops]   ##eliminate the stopwords (HINT: use List Comprehension)\n",
    "    line=[stemming.stem(word) for word in line]         ## perform stemming (HINT: use List Comprehension)\n",
    "    return line"
   ]
  },
  {
   "source": [
    "I used this function to write query results to a tsv file"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids2tsv(query,docs,file_name):\n",
    "    with open(file_name, 'a+', encoding='utf8', newline='') as tsv_file:\n",
    "        tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "        tsv_writer.writerow([ query])\n",
    "        for d_id in docs:\n",
    "            text = df_tweets['text'][d_id]\n",
    "            screen_name = df_tweets['user'][d_id]['screen_name']\n",
    "            date = df_tweets['created_at'][d_id]\n",
    "            hastags = df_tweets['hashtags'][d_id]\n",
    "            likes = df_tweets['likes'][d_id]\n",
    "            retweets = df_tweets['retweets'][d_id]\n",
    "            followers = df_tweets[\"user\"][d_id]['followers_count']\n",
    "            user_created = df_tweets[\"user\"][d_id]['created_at']\n",
    "            tweetid =  df_tweets['id'][d_id]\n",
    "            url = \"https://twitter.com/{}/status/{}\".format(screen_name,tweetid)\n",
    "            tsv_writer.writerow((\"\", text, screen_name, date,hastags, likes, retweets, url, tweet2label[d_id] ))\n",
    "        tsv_writer.writerow([\"\", \"\"])\n",
    "    "
   ]
  },
  {
   "source": [
    "## Clustering"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "For clustering, I used word2vec representation as in question 1"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = []\n",
    "for terms in df_tweets['processed_text']:\n",
    "    for word in terms:\n",
    "        all_words.append(nltk.word_tokenize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(all_words, size=24, min_count=1)"
   ]
  },
  {
   "source": [
    "Generate tweets vector based on word2vec representation"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2vec = {}\n",
    "tweetvecList = []\n",
    "key = 0\n",
    "for terms in df_tweets['processed_text']:\n",
    "    tweet2vec[key] = 0\n",
    "    for word in terms:\n",
    "        try:\n",
    "            tweet2vec[key] += (word2vec[word])\n",
    "        except:\n",
    "            continue\n",
    "    if(len(terms) > 0):\n",
    "        tweet2vec[key] = tweet2vec[key] / len(terms)\n",
    "        tweet2vec[key] = (tweet2vec[key])\n",
    "        tweetvecList.append(tweet2vec[key])\n",
    "    key+=1"
   ]
  },
  {
   "source": [
    "Generating clusters"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=3, random_state=0).fit(tweetvecList)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2tweets=defaultdict(lambda: [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for pos, label in enumerate(labels):\n",
    "    labels2tweets[label].append(pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words_from_ids(ids):\n",
    "    bag_of_words = {}\n",
    "    for key in ids:\n",
    "        lst_text = df_tweets['processed_text'][key]\n",
    "        # create bag-of-words - for each word the frequency of the word in the corpus\n",
    "        for w in lst_text:\n",
    "            if w not in bag_of_words:\n",
    "                bag_of_words[w] = 0\n",
    "            bag_of_words[w]+=1\n",
    "\n",
    "    return bag_of_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_head(bag):\n",
    "    words_obj = [{\"word\":word, \"count\": bag[word]} for word in bag]\n",
    "\n",
    "    counter_words = pd.DataFrame(words_obj)\n",
    "\n",
    "    counter_words.sort_values(\"count\", inplace=True, ascending=False)\n",
    "\n",
    "    total_count = 0\n",
    "    counter_words = counter_words.head(20)\n",
    "    for index, row in counter_words.iterrows():\n",
    "        total_count += row['count']\n",
    "    word2norm = []\n",
    "    for index, row in counter_words.iterrows():\n",
    "        word2norm.append(row['count'] / total_count)\n",
    "    counter_words['norm'] = word2norm\n",
    "    return counter_words"
   ]
  },
  {
   "source": [
    "Generating cluster label to terms head"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "label2head = {}\n",
    "for l in labels2tweets:\n",
    "    bag_of_words =  bag_of_words_from_ids(labels2tweets[l])\n",
    "    label2head[l] = get_head(bag_of_words)"
   ],
   "cell_type": "code",
   "metadata": {},
   "execution_count": 73,
   "outputs": []
  },
  {
   "source": [
    "Dictionary for storing get the cluster label from tweet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweet2label = {}\n",
    "for index, row in df_tweets.iterrows():\n",
    "    tweet2label[index] = -1\n",
    "    for label in labels2tweets:\n",
    "        if(index in labels2tweets[label]):\n",
    "            tweet2label[index] = label\n",
    "            break"
   ]
  },
  {
   "source": [
    "## Diversity Score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "Diversity score function"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_diversity_score():\n",
    "    ds = defaultdict(list)\n",
    "    #iterate rows\n",
    "    for index, row in df_tweets.iterrows():\n",
    "        score = []\n",
    "        label = tweet2label[index]\n",
    "        if(label != -1): \n",
    "            score.append(label)\n",
    "            head = label2head[label]\n",
    "            ds_terms = {}\n",
    "            #iterate terms in eact tweet and compute term frequency fro terms in the cluster head\n",
    "            for term in row['processed_text']:\n",
    "                if(term in list(head['word'])):\n",
    "                    if(term in ds_terms):\n",
    "                        ds_terms[term] += 1\n",
    "                    else:\n",
    "                        ds_terms[term] = 1\n",
    "            #iterate the terms from before and normalize them by multiplying with the normalization compute for each term\n",
    "            for i, row in head.iterrows():\n",
    "                if(row['word'] in ds_terms):\n",
    "                    ds_terms[row['word']] = ds_terms[row['word']] * row['norm']\n",
    "            tweetds = 0\n",
    "            #sum all the scores and divide by number of head terms' in the tweet\n",
    "            for term in ds_terms:\n",
    "                tweetds += ds_terms[term]\n",
    "            score.append(tweetds / len(ds_terms))\n",
    "        else:\n",
    "            #if label no from any cluster\n",
    "            score = [-1,0]\n",
    "        ds[index] = score\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ds = compute_diversity_score()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index_tfidf(df_tweets):\n",
    "    numDocuments = len(df_tweets)\n",
    "    index=defaultdict(list)\n",
    "\n",
    "    tf=defaultdict(list) \n",
    "    df=defaultdict(int)        \n",
    "    idf=defaultdict(float)\n",
    "\n",
    "\n",
    "    for i, row in df_tweets.iterrows():\n",
    "        terms = row['processed_text']\n",
    "\n",
    "        page_id =  i\n",
    "        termdictPage={}\n",
    "\n",
    "        for position, term in enumerate(terms):\n",
    "            try:\n",
    "                # if the term is already in the dict append the position to the corrisponding list\n",
    "                termdictPage[term][1].append(position) \n",
    "            except:\n",
    "                # Add the new term as dict key and initialize the array of positions and add the position\n",
    "                termdictPage[term]=[page_id, array('I',[position])] \n",
    "        \n",
    "        norm=0\n",
    "        for term, posting in termdictPage.items(): \n",
    "            norm+=len(posting[1])**2\n",
    "        norm=math.sqrt(norm)\n",
    "\n",
    "        #calculate the tf(dividing the term frequency by the above computed norm) and df weights\n",
    "        for term, posting in termdictPage.items():   \n",
    "            # append the tf for current term (tf = term frequency in current doc/norm)\n",
    "            tf[term].append(np.round(len(posting[1])/norm,4))  ## SEE formula (1) above\n",
    "            #increment the document frequency of current term (number of documents containing the current term)\n",
    "            df[term] = df[term] + 1  \n",
    "\n",
    "        for termpage, postingpage in termdictPage.items():\n",
    "            index[termpage].append(postingpage)\n",
    "            \n",
    "    for term in df:\n",
    "        idf[term] = np.round(np.log(float(numDocuments/df[term])),4)\n",
    "            \n",
    "    return index, tf, df, idf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "index, tf, df, idf = create_index_tfidf(df_tweets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_from_query(query):\n",
    "    labels = []\n",
    "    for term in query:\n",
    "        for label in label2head:\n",
    "            for index, row in label2head[label].iterrows():\n",
    "                if(term in row['word'] and label not in labels):\n",
    "                    labels.append(label)\n",
    "                        \n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rankDocuments(terms, docs, index, idf, tf):\n",
    "    docVectors=defaultdict(lambda: [0]*len(terms)) \n",
    "\n",
    "    queryVector=[0]*len(terms)    \n",
    "\n",
    "    # compute the norm for the query tf\n",
    "    query_terms_count = collections.Counter(terms) \n",
    "    \n",
    "    query_norm = la.norm(list(query_terms_count.values()))\n",
    "\n",
    "    for termIndex, term in enumerate(terms): \n",
    "        if term not in index:\n",
    "            continue\n",
    "        queryVector[termIndex]=query_terms_count[term]/query_norm * idf[term] \n",
    "\n",
    "        # Generate docVectors for matching docs\n",
    "        for docIndex, (doc, postings) in enumerate(index[term]):\n",
    "               \n",
    "            if doc in docs:\n",
    "                docVectors[doc][termIndex]=tf[term][docIndex] * idf[term] \n",
    "    docScores=[ [np.dot(curDocVec, queryVector), doc] for doc, curDocVec in docVectors.items() ]\n",
    "\n",
    "    docScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docScores]\n",
    "\n",
    "    if len(resultDocs) == 0:\n",
    "        print(\"No results found, try again\")\n",
    "        query = input()\n",
    "        docs = search_tf_idf(query, index)    \n",
    "    \n",
    "    return resultDocs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tf_idf(query, index):\n",
    "    '''\n",
    "    output is the list of documents that contain any of the query terms. \n",
    "    So, we will get the list of documents for each query term, and take the union of them.\n",
    "    '''\n",
    "    query=processQuery(query)\n",
    "    docs=set()\n",
    "    for term in query:\n",
    "        try:\n",
    "            # store in termDocs the ids of the docs that contain \"term\"                        \n",
    "            termDocs=[posting[0] for posting in index[term]]\n",
    "            \n",
    "            # docs = docs Union termDocs\n",
    "            docs|=set(termDocs)\n",
    "        except:\n",
    "            #term is not in index\n",
    "            pass\n",
    "    docs=list(docs)\n",
    "    ranked_docs = rankDocuments(query, docs, index, idf, tf)   \n",
    "    return ranked_docs"
   ]
  },
  {
   "source": [
    "Funciton to rerank based on the diversity score"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def diversification(rankedDocs):\n",
    "    docDivScores = [[ds[d_id][1], d_id] for d_id in rankedDocs]\n",
    "    print(docDivScores)\n",
    "    docDivScores.sort(reverse=True)\n",
    "    resultDocs=[x[1] for x in docDivScores]\n",
    "    return resultDocs"
   ]
  },
  {
   "source": [
    "## Write tsv files"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries=[\n",
    "    \"us lockdown\",\n",
    "    \"trump has covid\",\n",
    "    \"holidays with covid\",\n",
    "    \"the pandemic in us\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_nameRQ2A_no_ds= \"../other-outputs/RQ2A_no_ds.tsv\"\n",
    "file_nameRQ2A_ds=\"../other-outputs/RQ2A_ds.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_nameRQ2A_no_ds, 'a+', encoding='utf8', newline='') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Query\",\"Tweet\", \"Username\", \"Date\", \"Hashtags\",\"Likes\",\"Retweets\",\"Url\",\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "us lockdown\n",
      "trump has covid\n",
      "holidays with covid\n",
      "the pandemic in us\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(query)\n",
    "    ranked_docs = search_tf_idf(query, index)\n",
    "    ids2tsv(query, ranked_docs[:20], file_nameRQ2A_no_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_nameRQ2A_ds, 'a+', encoding='utf8', newline='') as tsv_file:\n",
    "    tsv_writer = csv.writer(tsv_file, delimiter='\\t')\n",
    "    tsv_writer.writerow([\"Query\",\"Tweet\", \"Username\", \"Date\", \"Hashtags\",\"Likes\",\"Retweets\",\"Url\",\"Cluster\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "us lockdown\n",
      "trump has covid\n",
      "holidays with covid\n",
      "the pandemic in us\n"
     ]
    }
   ],
   "source": [
    "for query in queries:\n",
    "    print(query)\n",
    "    ranked_docs = search_tf_idf(query, index)\n",
    "    ranked_docs = diversification(ranked_docs)\n",
    "    ids2tsv(query, ranked_docs[:20], file_nameRQ2A_ds)"
   ]
  }
 ]
}